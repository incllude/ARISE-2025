{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:27:36.692040Z",
     "iopub.status.busy": "2025-03-12T14:27:36.691704Z",
     "iopub.status.idle": "2025-03-12T14:27:37.617448Z",
     "shell.execute_reply": "2025-03-12T14:27:37.616784Z",
     "shell.execute_reply.started": "2025-03-12T14:27:36.692011Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:27:38.520085Z",
     "iopub.status.busy": "2025-03-12T14:27:38.519627Z",
     "iopub.status.idle": "2025-03-12T14:27:40.608092Z",
     "shell.execute_reply": "2025-03-12T14:27:40.607143Z",
     "shell.execute_reply.started": "2025-03-12T14:27:38.520058Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/ARISE-2025/* .\n",
    "!chmod +x preprocess_for_yolo.sh\n",
    "!chmod +x preprocess_for_classifier.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T14:27:40.609631Z",
     "iopub.status.busy": "2025-03-12T14:27:40.609362Z",
     "iopub.status.idle": "2025-03-12T14:31:26.429031Z",
     "shell.execute_reply": "2025-03-12T14:31:26.428182Z",
     "shell.execute_reply.started": "2025-03-12T14:27:40.609598Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for s4torch (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -r requirements.txt\n",
    "!pip install -q monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:26.431459Z",
     "iopub.status.busy": "2025-03-12T14:31:26.431173Z",
     "iopub.status.idle": "2025-03-12T14:31:26.437282Z",
     "shell.execute_reply": "2025-03-12T14:31:26.436494Z",
     "shell.execute_reply.started": "2025-03-12T14:31:26.431436Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/train.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/train.yaml\n",
    "# Model configuration\n",
    "random_state: 43\n",
    "defaults:\n",
    "  - model: timm\n",
    "  - optimizer: adamw\n",
    "  - train_dataset: base_ds\n",
    "  - val_dataset: base_ds\n",
    "  - scheduler: step_lr\n",
    "\n",
    "# Training configuration\n",
    "training:\n",
    "  normalized_jsn_class_weights: [0.363, 0.624, 0.272, 0.575, 3.827]\n",
    "  normalized_erosion_class_weights: [0.012, 0.183, 0.391, 2.571, 7.714, 1.058]\n",
    "  num_workers: 4\n",
    "  epochs: 10\n",
    "  batch_size: 16\n",
    "  save_dir: \"/kaggle/working/checkpoints/classification\"  \n",
    "\n",
    "# WandB configuration\n",
    "wandb:\n",
    "  api_key: \"\"\n",
    "  project: \"ARISE-2025\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T16:36:31.983945Z",
     "iopub.status.busy": "2025-03-12T16:36:31.983634Z",
     "iopub.status.idle": "2025-03-12T16:36:31.990019Z",
     "shell.execute_reply": "2025-03-12T16:36:31.989170Z",
     "shell.execute_reply.started": "2025-03-12T16:36:31.983921Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /kaggle/working/config/classification/model/timm.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/model/timm.yaml\n",
    "_target_: classification.models.timms.timm_Model\n",
    "model_name: \"timm/convnext_tiny.in12k\"\n",
    "n_output_layers: 2\n",
    "output_layers_shapes:\n",
    "  6: \"erosion\"\n",
    "  5: \"jsn\"\n",
    "pretrained: True\n",
    "drop_rate: 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/model/rnn.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/model/rnn.yaml\n",
    "_target_: classification.models.rnn.S4ImageClassifier\n",
    "img_height: 128\n",
    "img_width: 128\n",
    "d_model: 128\n",
    "n_blocks: 4\n",
    "dropout: 0.2\n",
    "num_classes1: 5\n",
    "num_classes2: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T15:30:34.449877Z",
     "iopub.status.busy": "2025-03-12T15:30:34.449586Z",
     "iopub.status.idle": "2025-03-12T15:30:34.455479Z",
     "shell.execute_reply": "2025-03-12T15:30:34.454595Z",
     "shell.execute_reply.started": "2025-03-12T15:30:34.449853Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/optimizer/adamw.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/optimizer/adamw.yaml\n",
    "_target_: torch.optim.AdamW\n",
    "head_lr: 3e-4\n",
    "lr: 5e-05\n",
    "weight_decay: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:26.491526Z",
     "iopub.status.busy": "2025-03-12T14:31:26.491314Z",
     "iopub.status.idle": "2025-03-12T14:31:26.511598Z",
     "shell.execute_reply": "2025-03-12T14:31:26.510943Z",
     "shell.execute_reply.started": "2025-03-12T14:31:26.491507Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/scheduler/step_lr.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/scheduler/step_lr.yaml\n",
    "_target_: torch.optim.lr_scheduler.StepLR\n",
    "step_size: 2\n",
    "gamma: 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:26.513728Z",
     "iopub.status.busy": "2025-03-12T14:31:26.513523Z",
     "iopub.status.idle": "2025-03-12T14:31:26.530702Z",
     "shell.execute_reply": "2025-03-12T14:31:26.530028Z",
     "shell.execute_reply.started": "2025-03-12T14:31:26.513702Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/classification/models/timms.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/classification/models/timms.py\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from torch import nn\n",
    "import timm\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "class timm_Model(nn.Module):\n",
    "    def __init__(self,\n",
    "                 model_name,\n",
    "                 drop_rate=0.0,\n",
    "                 n_output_layers=2,\n",
    "                 output_layers_shapes={\n",
    "                    5: \"erosion\",\n",
    "                    4: \"jsn\"\n",
    "                 },\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_mapping = output_layers_shapes\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained, drop_rate=drop_rate)\n",
    "        self.heads = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(self.backbone.head.fc.in_features, self.backbone.head.fc.in_features // 2),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(self.backbone.head.fc.in_features // 2, key)\n",
    "            )\n",
    "            for key in self.output_mapping.keys()\n",
    "        ])\n",
    "        self.backbone.head = nn.Identity()\n",
    "            \n",
    "        self.heads.apply(init_weights)\n",
    "    \n",
    "        \n",
    "    def forward(self, x):\n",
    "        feats = self.backbone(x)\n",
    "        feats = feats[:, :, 4, 4]\n",
    "        head_outputs = {\n",
    "            self.output_mapping[self.heads[i][-1].out_features]: self.heads[i](feats)\n",
    "            for i in range(len(self.heads))\n",
    "        }\n",
    "        return head_outputs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = timm_Model()\n",
    "    print(model(torch.randn((1, 3, 224, 224))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:56:32.259271Z",
     "iopub.status.busy": "2025-03-12T14:56:32.258937Z",
     "iopub.status.idle": "2025-03-12T14:56:32.265289Z",
     "shell.execute_reply": "2025-03-12T14:56:32.264571Z",
     "shell.execute_reply.started": "2025-03-12T14:56:32.259247Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/train_dataset/base_ds.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/train_dataset/base_ds.yaml\n",
    "_target_: data_utils.datasets.ImageClassificationDataset\n",
    "img_dir: \"/kaggle/working/data/classifier_data/train\"\n",
    "transform:\n",
    "  _target_: monai.transforms.Compose\n",
    "  transforms:\n",
    "    - _target_: monai.transforms.LoadImage\n",
    "    - _target_: data_utils.datasets.Processor\n",
    "      clip_limit: 1.0\n",
    "    - _target_: data_utils.datasets.DynamicSquarePad\n",
    "    - _target_: monai.transforms.Resize\n",
    "      spatial_size: [224, 224]\n",
    "      mode: bilinear\n",
    "    - _target_: monai.transforms.RandFlip\n",
    "      prob: 0.5\n",
    "      spatial_axis: 1\n",
    "    - _target_: monai.transforms.NormalizeIntensity\n",
    "    - _target_: monai.transforms.EnsureType\n",
    "      data_type: tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:56:37.895075Z",
     "iopub.status.busy": "2025-03-12T14:56:37.894765Z",
     "iopub.status.idle": "2025-03-12T14:56:37.900308Z",
     "shell.execute_reply": "2025-03-12T14:56:37.899378Z",
     "shell.execute_reply.started": "2025-03-12T14:56:37.895051Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/config/classification/val_dataset/base_ds.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/config/classification/val_dataset/base_ds.yaml\n",
    "_target_: data_utils.datasets.ImageClassificationDataset\n",
    "img_dir: \"/kaggle/working/data/classifier_data/val\"\n",
    "transform:\n",
    "  _target_: monai.transforms.Compose\n",
    "  transforms:\n",
    "    - _target_: monai.transforms.LoadImage\n",
    "    - _target_: data_utils.datasets.Processor\n",
    "      clip_limit: 1.0\n",
    "    - _target_: data_utils.datasets.DynamicSquarePad\n",
    "    - _target_: monai.transforms.Resize\n",
    "      spatial_size: [224, 224]\n",
    "      mode: bilinear\n",
    "    - _target_: monai.transforms.NormalizeIntensity\n",
    "    - _target_: monai.transforms.EnsureType\n",
    "      data_type: tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:26.569576Z",
     "iopub.status.busy": "2025-03-12T14:31:26.569287Z",
     "iopub.status.idle": "2025-03-12T14:31:26.587756Z",
     "shell.execute_reply": "2025-03-12T14:31:26.586900Z",
     "shell.execute_reply.started": "2025-03-12T14:31:26.569543Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/data_utils/datasets.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/data_utils/datasets.py\n",
    "from monai.transforms import Compose, SpatialPad\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from hydra.utils import instantiate\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "class DynamicSquarePad:\n",
    "    def __call__(self, img):\n",
    "        # Find the largest spatial dimension\n",
    "        spatial_shape = img.shape[1:]  # Assuming channel-first data\n",
    "        max_dim = max(spatial_shape)\n",
    "        \n",
    "        # Create a spatial pad transform with square dimensions\n",
    "        pad = SpatialPad(spatial_size=[max_dim] * len(spatial_shape))\n",
    "        return pad(img)\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, clip_limit=2.0, tile_grid_size=(8, 8)):\n",
    "        \"\"\"\n",
    "        Initialize CLAHE processor.\n",
    "        \n",
    "        Args:\n",
    "            clip_limit: Threshold for contrast limiting\n",
    "            tile_grid_size: Size of grid for histogram equalization\n",
    "        \"\"\"\n",
    "        self.clip_limit = clip_limit\n",
    "        self.tile_grid_size = tile_grid_size\n",
    "        self.clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Apply CLAHE to an image.\n",
    "        \n",
    "        Args:\n",
    "            img: NumPy array in HWC format (Height, Width, Channels) or grayscale\n",
    "            \n",
    "        Returns:\n",
    "            Processed image in the same format as input\n",
    "        \"\"\"\n",
    "        res = self.clahe.apply(img[:, :, 0].numpy().astype(np.uint8)).T\n",
    "        res = np.stack((res, res, res), axis=0).astype(float) / 255\n",
    "        return torch.from_numpy(res).double()\n",
    "\n",
    "\n",
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform):\n",
    "        self.img_dir = img_dir\n",
    "        self.entries = os.listdir(img_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.entries[idx])\n",
    "        erosion_score, jsn_score = os.path.splitext(self.entries[idx])[0].split(\"_\")[-2:]\n",
    "\n",
    "        img = self.transform(img_path)\n",
    "        return img, int(erosion_score), int(jsn_score)\n",
    "\n",
    "\n",
    "class EvalImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, bbox_csv, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.bbox_df = pd.read_csv(bbox_csv)\n",
    "        self.samples = self._load_samples()\n",
    "\n",
    "    def _load_samples(self):\n",
    "        samples = []\n",
    "        for _, row in self.bbox_df.iterrows():\n",
    "            patient_id = row[\"patient_id\"]\n",
    "            joint_id = row[\"joint_id\"]\n",
    "            image_name = f\"{int(patient_id)}_{int(joint_id)}.jpeg\"\n",
    "            image_path = os.path.join(self.image_dir, image_name)\n",
    "            if os.path.exists(image_path):\n",
    "                samples.append((image_path, patient_id, joint_id, row[\"xcenter\"], row[\"ycenter\"], row[\"dx\"], row[\"dy\"]))\n",
    "        return samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, patient_id, joint_id, xcenter, ycenter, dx, dy = self.samples[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image_path)\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"patient_id\": patient_id,\n",
    "            \"joint_id\": joint_id,\n",
    "            \"xcenter\": xcenter,\n",
    "            \"ycenter\": ycenter,\n",
    "            \"dx\": dx,\n",
    "            \"dy\": dy,\n",
    "        }\n",
    "\n",
    "\n",
    "def initialize_data(cfg):\n",
    "    train_dataset = instantiate(cfg.train_dataset)\n",
    "    val_dataset = instantiate(cfg.val_dataset)\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=cfg.training.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=cfg.training.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=cfg.training.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:37:35.275801Z",
     "iopub.status.busy": "2025-03-12T14:37:35.275449Z",
     "iopub.status.idle": "2025-03-12T14:37:35.282763Z",
     "shell.execute_reply": "2025-03-12T14:37:35.281783Z",
     "shell.execute_reply.started": "2025-03-12T14:37:35.275774Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /kaggle/working/classification/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile /kaggle/working/classification/train.py\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from accelerate import Accelerator\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "import wandb\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "from tqdm import tqdm\n",
    "from metrics import calculate_accuracy\n",
    "from data_utils.datasets import initialize_data\n",
    "from classification.utils import save_checkpoint\n",
    "from accelerate.utils import set_seed\n",
    "import torch.nn.functional as F\n",
    "import monai\n",
    "\n",
    "\n",
    "set_seed(43)\n",
    "monai.utils.set_determinism(seed=43)\n",
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, gamma=2, weights=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.weights = weights\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)\n",
    "        weights = self.weights.to(inputs.device).gather(0, targets.view(-1))\n",
    "        loss = weights * (1-pt)**self.gamma * ce_loss\n",
    "\n",
    "        return loss.sum() / weights.sum()\n",
    "\n",
    "\n",
    "def train_epoch(cfg, model, train_loader, optimizer, criterion_erosion, criterion_jsn, accelerator):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
    "        img, erosion_score, jsn_score = batch\n",
    "        img = img.double()\n",
    "        \n",
    "        outputs = model(img)\n",
    "        \n",
    "        loss_jsn = criterion_jsn(outputs[\"jsn\"], jsn_score)\n",
    "        loss_erosion = criterion_erosion(outputs[\"erosion\"], erosion_score)\n",
    "        loss = loss_jsn + loss_erosion\n",
    "\n",
    "        accelerator.backward(loss)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Log to WandB\n",
    "        if accelerator.is_local_main_process:\n",
    "            wandb.log({\n",
    "                \"train_loss\": loss.item(),\n",
    "                \"train_loss_jsn\": loss_jsn.item(),\n",
    "                \"train_loss_erosion\": loss_erosion.item(),\n",
    "                \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "            })\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Validation function\n",
    "def validate_epoch(cfg, model, val_loader, criterion_erosion, criterion_jsn, accelerator):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    jsn_accuracy, erosion_accuracy = 0.0, 0.0\n",
    "\n",
    "    jsn_res = []\n",
    "    jsn_labels = []\n",
    "    erosion_res = []\n",
    "    erosion_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            img, erosion_score, jsn_score = batch\n",
    "            img = img.double()\n",
    "            \n",
    "            outputs = model(img)\n",
    "            \n",
    "            loss_jsn = criterion_jsn(outputs[\"jsn\"], jsn_score)\n",
    "            loss_erosion = criterion_erosion(outputs[\"erosion\"], erosion_score)\n",
    "            loss = loss_jsn + loss_erosion\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            jsn_res.extend(outputs[\"jsn\"].detach().cpu().argmax(dim=1).tolist())\n",
    "            jsn_labels.extend(jsn_score.detach().cpu().tolist())\n",
    "            \n",
    "            erosion_res.extend(outputs[\"erosion\"].detach().cpu().argmax(dim=1).tolist())\n",
    "            erosion_labels.extend(erosion_score.detach().cpu().tolist())\n",
    "            \n",
    "            # Calculate weighted-accuracy\n",
    "        jsn_accuracy = balanced_accuracy_score(jsn_labels, jsn_res)\n",
    "        erosion_accuracy = balanced_accuracy_score(erosion_labels, erosion_res)\n",
    "\n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "\n",
    "    # Log to WandB\n",
    "    if accelerator.is_local_main_process:\n",
    "        wandb.log({\n",
    "            \"val_loss\": avg_loss,\n",
    "            \"val_jsn_accuracy\": jsn_accuracy,\n",
    "            \"val_erosion_accuracy\": erosion_accuracy\n",
    "        })\n",
    "\n",
    "    return avg_loss, jsn_accuracy, erosion_accuracy\n",
    "\n",
    "\n",
    "@hydra.main(config_path=\"../config/classification\", config_name=\"train\")\n",
    "def main(cfg: DictConfig):\n",
    "\n",
    "    if accelerator.is_local_main_process:\n",
    "        os.environ[\"WANDB_API_KEY\"] = cfg.wandb.api_key\n",
    "        wandb.init(project=cfg.wandb.project, config=OmegaConf.to_container(cfg, resolve=True))\n",
    "\n",
    "    model = instantiate(cfg.model).double()\n",
    "\n",
    "    \n",
    "    # optimizer = instantiate(cfg.optimizer, params=model.parameters())\n",
    "    optimizer = eval(cfg.optimizer._target_)([\n",
    "        {\"params\": model.heads.parameters(), \"lr\": cfg.optimizer.head_lr, \"weight_decay\": cfg.optimizer.weight_decay},\n",
    "        {\"params\": model.backbone.parameters(), \"lr\": cfg.optimizer.lr, \"weight_decay\": cfg.optimizer.weight_decay}\n",
    "    ])\n",
    "    scheduler = instantiate(cfg.scheduler, optimizer=optimizer)\n",
    "    train_loader, val_loader = initialize_data(cfg)\n",
    "\n",
    "    criterion_erosion = nn.CrossEntropyLoss(weight=torch.tensor(cfg.training.normalized_erosion_class_weights).double().cuda())\n",
    "    criterion_jsn = nn.CrossEntropyLoss(weight=torch.tensor(cfg.training.normalized_jsn_class_weights).double().cuda())\n",
    "    \n",
    "    model, optimizer, train_loader, val_loader = accelerator.prepare(\n",
    "        model, optimizer, train_loader, val_loader\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(cfg.training.epochs):\n",
    "        train_loss = train_epoch(cfg, model, train_loader, optimizer, criterion_erosion, criterion_jsn, accelerator)\n",
    "        val_loss, jsn_accuracy, erosion_accuracy = validate_epoch(cfg, model, val_loader, criterion_erosion, criterion_jsn, accelerator)\n",
    "\n",
    "        # Step the scheduler\n",
    "        scheduler.step()\n",
    "\n",
    "        # Print epoch results\n",
    "        if accelerator.is_local_main_process:\n",
    "            print(f\"Epoch {epoch + 1}/{cfg.training.epochs}\")\n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"JSN Accuracy: {jsn_accuracy:.4f}\")\n",
    "            print(f\"Erosion Accuracy: {erosion_accuracy:.4f}\")\n",
    "            \n",
    "        if accelerator.is_local_main_process:\n",
    "            save_checkpoint(model, epoch + 1, jsn_accuracy, erosion_accuracy, cfg.training.save_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:26.606594Z",
     "iopub.status.busy": "2025-03-12T14:31:26.606330Z",
     "iopub.status.idle": "2025-03-12T14:31:42.623230Z",
     "shell.execute_reply": "2025-03-12T14:31:42.622300Z",
     "shell.execute_reply.started": "2025-03-12T14:31:26.606573Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PYTHONPATH: /kaggle/lib/kagglegym:/kaggle/lib:/kaggle/working\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/2.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/3.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/4.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/5.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/8.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/9.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/11.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/15.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/16.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/17.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/18.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/19.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/24.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/30.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/31.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/51.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/56.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/62.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/65.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/74.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/78.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/85.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/94.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/96.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/103.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/109.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/121.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/124.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/125.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/126.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/131.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/142.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/144.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/148.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/151.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/154.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/155.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/156.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/157.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/158.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/159.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/161.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/163.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/165.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/166.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/167.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/168.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/170.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/171.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/172.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/174.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/175.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/176.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/177.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/178.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/179.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/180.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/181.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/182.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/189.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/198.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/200.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/201.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/203.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/204.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/206.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/207.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/208.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/209.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/211.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/212.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/213.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/214.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/215.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/216.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/217.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/221.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/222.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/223.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/224.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/225.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/226.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/227.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/229.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/230.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/231.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/232.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/233.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/235.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/236.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/237.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/239.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/240.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/241.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/243.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/244.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/245.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/246.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/248.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/249.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/251.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/252.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/253.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/254.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/255.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/256.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/258.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/260.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/261.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/263.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/264.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/266.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/267.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/268.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/270.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/271.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/273.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/274.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/275.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/276.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/277.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/278.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/279.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/282.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/285.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/287.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/290.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/291.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/292.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/293.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/295.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/296.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/297.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/299.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/300.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/301.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/302.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/303.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/304.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/305.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/306.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/307.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/308.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/309.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/310.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/311.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/312.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/313.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/315.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/316.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/317.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/318.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/319.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/320.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/322.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/323.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/324.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/325.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/326.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/327.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/328.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/329.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/330.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/331.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/332.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/334.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/335.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/336.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/337.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/338.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/339.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/340.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/341.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/342.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/343.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/344.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/345.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/346.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/347.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/348.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/349.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/350.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/351.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/352.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/353.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/354.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/356.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/357.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/358.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/359.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/360.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/361.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/362.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/363.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/364.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/365.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/366.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/367.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/368.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/371.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/372.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/373.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/374.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/375.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/376.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/377.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/379.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/380.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/382.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/383.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/384.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/386.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/388.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/389.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/390.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/391.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/392.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/395.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/396.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/397.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/398.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/399.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/400.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/401.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/403.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/404.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/405.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/406.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/407.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/408.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/409.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/410.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/411.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/412.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/413.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/414.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/415.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/416.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/417.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/418.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/419.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/420.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/421.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/422.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/423.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/424.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/425.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/426.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/427.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/428.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/429.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/430.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/431.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/432.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/433.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/434.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/435.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/436.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/437.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/438.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/439.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/440.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/441.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/442.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/443.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/444.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/445.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/446.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/447.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/448.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/450.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/452.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/454.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/455.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/456.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/457.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/458.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/460.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/461.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/462.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/463.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/464.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/466.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/467.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/468.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/469.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/470.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/471.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/472.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/473.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/474.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/475.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/476.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/477.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/478.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/479.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/482.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/val/483.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/484.txt\n",
      "Created YOLO annotation file: data/yolo_dataset/labels/train/485.txt\n"
     ]
    }
   ],
   "source": [
    "!./preprocess_for_yolo.sh \\\n",
    "--csv /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/bboxes.csv \\\n",
    "--img_dir /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/jpeg \\\n",
    "--split_info_path /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/train_val_split.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-12T14:31:42.624643Z",
     "iopub.status.busy": "2025-03-12T14:31:42.624273Z",
     "iopub.status.idle": "2025-03-12T14:37:00.556830Z",
     "shell.execute_reply": "2025-03-12T14:37:00.555731Z",
     "shell.execute_reply.started": "2025-03-12T14:31:42.624618Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PYTHONPATH: /kaggle/lib/kagglegym:/kaggle/lib:/kaggle/working\n",
      "Running data_utils/average_scores.py...\n",
      "Input CSV: \n",
      "Output CSV: data/averaged_scores.csv\n",
      "Transformed data saved to data/averaged_scores.csv\n",
      "Averaged scores saved to data/averaged_scores.csv\n",
      "Running data_utils/merge_scores_bbox_files.py...\n",
      "Bounding Box File: /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/bboxes.csv\n",
      "Score File: data/averaged_scores.csv\n",
      "Output File: data/merged_score_file.csv\n",
      "Merged scores and bounding boxes saved to data/merged_score_file.csv\n",
      "Running data_utils/crop_images.py...\n",
      "Label File: data/merged_score_file.csv\n",
      "Image Directory: /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/jpeg\n",
      "Output Directory: data/classifier_data\n",
      "Split Subsets By ID: /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/train_val_split.json\n",
      "Cropped images saved to data/classifier_data\n",
      "All steps completed successfully!\n"
     ]
    }
   ],
   "source": [
    "!./preprocess_for_classifier.sh \\\n",
    "--scores_csv /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/scores.csv \\\n",
    "--bbox_file /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/bboxes.csv \\\n",
    "--image_dir /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/jpeg \\\n",
    "--split_subsets_by_id /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/train_val_split.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T14:37:00.558085Z",
     "iopub.status.busy": "2025-03-12T14:37:00.557805Z",
     "iopub.status.idle": "2025-03-12T14:37:00.563715Z",
     "shell.execute_reply": "2025-03-12T14:37:00.562832Z",
     "shell.execute_reply.started": "2025-03-12T14:37:00.558062Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.sh\n",
    "export PYTHONPATH=$PYTHONPATH:/kaggle/working\n",
    "export HYDRA_FULL_ERROR=1\n",
    "echo \"Updated PYTHONPATH: $PYTHONPATH\"\n",
    "accelerate launch classification/train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-12T17:04:53.039Z",
     "iopub.execute_input": "2025-03-12T16:36:34.320799Z",
     "iopub.status.busy": "2025-03-12T16:36:34.320487Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated PYTHONPATH: /kaggle/lib/kagglegym:/kaggle/lib:/kaggle/working\n",
      "2025-03-12 23:07:43.284671: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-12 23:07:43.308959: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-12 23:07:43.315780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/kaggle/working/classification/train.py:125: UserWarning: \n",
      "The version_base parameter is not specified.\n",
      "Please specify a compatability version level, or None.\n",
      "Will assume defaults for version 1.1\n",
      "  @hydra.main(config_path=\"../config/classification\", config_name=\"train\")\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/defaults_list.py:251: UserWarning: In 'train': Defaults list is missing `_self_`. See https://hydra.cc/docs/1.2/upgrades/1.0_to_1.1/default_composition_order for more information\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "  ret = run_job(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mincllude\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/outputs/2025-03-12/23-07-46/wandb/run-20250312_230746-r74si05y\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvaliant-night-201\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/incllude/ARISE-2025\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/incllude/ARISE-2025/runs/r74si05y\u001b[0m\n",
      "[2025-03-12 23:07:48,349][timm.models._builder][INFO] - Loading pretrained weights from Hugging Face hub (timm/convnext_tiny.in12k)\n",
      "[2025-03-12 23:07:48,471][timm.models._hub][INFO] - [timm/convnext_tiny.in12k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:49<00:00,  3.71it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:16<00:00,  9.60it/s]\n",
      "Epoch 1/10\n",
      "Train Loss: 2.5409\n",
      "Val Loss: 2.2552\n",
      "JSN Accuracy: 0.4834\n",
      "Erosion Accuracy: 0.3188\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:48<00:00,  3.73it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:16<00:00,  9.58it/s]\n",
      "Epoch 2/10\n",
      "Train Loss: 2.0131\n",
      "Val Loss: 1.9473\n",
      "JSN Accuracy: 0.6268\n",
      "Erosion Accuracy: 0.3832\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:48<00:00,  3.73it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:17<00:00,  9.24it/s]\n",
      "Epoch 3/10\n",
      "Train Loss: 1.6382\n",
      "Val Loss: 1.9718\n",
      "JSN Accuracy: 0.6220\n",
      "Erosion Accuracy: 0.4228\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:48<00:00,  3.73it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:16<00:00,  9.41it/s]\n",
      "Epoch 4/10\n",
      "Train Loss: 1.5360\n",
      "Val Loss: 1.9006\n",
      "JSN Accuracy: 0.6610\n",
      "Erosion Accuracy: 0.4392\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:48<00:00,  3.73it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:16<00:00,  9.37it/s]\n",
      "Epoch 5/10\n",
      "Train Loss: 1.2715\n",
      "Val Loss: 1.8406\n",
      "JSN Accuracy: 0.6772\n",
      "Erosion Accuracy: 0.4394\n",
      "Training: 100%|███████████████████████████████| 630/630 [02:48<00:00,  3.73it/s]\n",
      "Validation: 100%|█████████████████████████████| 158/158 [00:16<00:00,  9.56it/s]\n",
      "Epoch 6/10\n",
      "Train Loss: 1.1530\n",
      "Val Loss: 1.9656\n",
      "JSN Accuracy: 0.6816\n",
      "Erosion Accuracy: 0.4088\n",
      "Training:  17%|█████                          | 104/630 [00:28<02:20,  3.75it/s]^C\n",
      "Training:  17%|█████                          | 104/630 [00:28<02:25,  3.62it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/classification/train.py\", line 182, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/main.py\", line 94, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py\", line 119, in run\n",
      "    ret = run_job(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/kaggle/working/classification/train.py\", line 163, in main\n",
      "    train_loss = train_epoch(cfg, model, train_loader, optimizer, criterion_erosion, criterion_jsn, accelerator)\n",
      "  File \"/kaggle/working/classification/train.py\", line 53, in train_epoch\n",
      "    outputs = model(img)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/kaggle/working/classification/models/timms.py\", line 58, in forward\n",
      "    feats = self.backbone(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 491, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 483, in forward_features\n",
      "    x = self.stages(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 233, in forward\n",
      "    x = self.blocks(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 158, in forward\n",
      "    x = self.norm(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/norm.py\", line 57, in forward\n",
      "    x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2901, in layer_norm\n",
      "    return handle_torch_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/overrides.py\", line 1742, in handle_torch_function\n",
      "    result = torch_func_method(public_api, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\", line 303, in __torch_function__\n",
      "    ret = MetaTensor.update_meta(ret, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\", line 218, in update_meta\n",
      "    ret.copy_meta_from(meta_args, copy_attr=not is_batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_obj.py\", line 128, in copy_meta_from\n",
      "    first_meta = input_objs if isinstance(input_objs, MetaObj) else first(input_objs, default=self)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/utils/misc.py\", line 136, in first\n",
      "    for i in iterable:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_obj.py\", line 101, in flatten_meta_objs\n",
      "    if isinstance(a, (list, tuple)):\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/classification/train.py\", line 182, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/main.py\", line 94, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py\", line 119, in run\n",
      "    ret = run_job(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "  File \"/kaggle/working/classification/train.py\", line 163, in main\n",
      "    train_loss = train_epoch(cfg, model, train_loader, optimizer, criterion_erosion, criterion_jsn, accelerator)\n",
      "  File \"/kaggle/working/classification/train.py\", line 53, in train_epoch\n",
      "    outputs = model(img)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/kaggle/working/classification/models/timms.py\", line 58, in forward\n",
      "    feats = self.backbone(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 491, in forward\n",
      "    x = self.forward_features(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 483, in forward_features\n",
      "    x = self.stages(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 233, in forward\n",
      "    x = self.blocks(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/models/convnext.py\", line 158, in forward\n",
      "    x = self.norm(x)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/timm/layers/norm.py\", line 57, in forward\n",
      "    x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\", line 2901, in layer_norm\n",
      "    return handle_torch_function(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/overrides.py\", line 1742, in handle_torch_function\n",
      "    result = torch_func_method(public_api, types, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\", line 303, in __torch_function__\n",
      "    ret = MetaTensor.update_meta(ret, func, args, kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_tensor.py\", line 218, in update_meta\n",
      "    ret.copy_meta_from(meta_args, copy_attr=not is_batch)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_obj.py\", line 128, in copy_meta_from\n",
      "    first_meta = input_objs if isinstance(input_objs, MetaObj) else first(input_objs, default=self)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/utils/misc.py\", line 136, in first\n",
      "    for i in iterable:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/monai/data/meta_obj.py\", line 101, in flatten_meta_objs\n",
      "    if isinstance(a, (list, tuple)):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!chmod +x train.sh\n",
    "!./train.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T19:30:12.624239Z",
     "iopub.status.busy": "2025-03-11T19:30:12.623892Z",
     "iopub.status.idle": "2025-03-11T19:30:22.013489Z",
     "shell.execute_reply": "2025-03-11T19:30:22.012451Z",
     "shell.execute_reply.started": "2025-03-11T19:30:12.624211Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python detection/model/yolo/infer.py \\\n",
    "--model checkpoints/detection/best.pt \\\n",
    "--img_dir /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/eval_data \\\n",
    "--output_dir /kaggle/working/data/eval_detection_output \\\n",
    "--imgsz 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T19:30:22.015055Z",
     "iopub.status.busy": "2025-03-11T19:30:22.014736Z",
     "iopub.status.idle": "2025-03-11T19:30:46.297647Z",
     "shell.execute_reply": "2025-03-11T19:30:46.296533Z",
     "shell.execute_reply.started": "2025-03-11T19:30:22.015025Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!python data_utils/crop_eval.py \\\n",
    "--image_dir /kaggle/input/automated-scoring-in-rheumatoid-arthritis/dataset/eval_data \\\n",
    "--bbox_txt data/eval_detection_output/bbox_txts/all_bboxes.txt \\\n",
    "--output_dir data/eval_cropped_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:45:34.586265Z",
     "iopub.status.busy": "2025-03-11T15:45:34.585858Z",
     "iopub.status.idle": "2025-03-11T15:45:34.593018Z",
     "shell.execute_reply": "2025-03-11T15:45:34.592294Z",
     "shell.execute_reply.started": "2025-03-11T15:45:34.586230Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile submit.py\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from monai import transforms\n",
    "from PIL import Image\n",
    "import hydra\n",
    "from omegaconf import DictConfig, OmegaConf\n",
    "from hydra.utils import instantiate\n",
    "from classification.utils import load_eval_model\n",
    "from data_utils.datasets import EvalImageDataset, DynamicSquarePad\n",
    "\n",
    "\n",
    "def predict_scores(cfg, model, dataloader, device):\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            outputs = model(batch[\"image\"].double())\n",
    "            jsn_scores = outputs[\"jsn\"].argmax(dim=1).cpu().numpy()  # Probability of positive class\n",
    "            erosion_scores = outputs[\"erosion\"].argmax(dim=1).cpu().numpy()  # Probability of positive class\n",
    "\n",
    "            for i in range(len(batch[\"image\"])):\n",
    "                results.append({\n",
    "                    \"ID\": f'{int(batch[\"patient_id\"][i].item())}_{int(batch[\"joint_id\"][i].item())}',\n",
    "                    \"patient_id\": int(batch[\"patient_id\"][i].item()),\n",
    "                    \"joint_id\": int(batch[\"joint_id\"][i].item()),\n",
    "                    \"xcenter\": batch[\"xcenter\"][i].item(),\n",
    "                    \"ycenter\": batch[\"ycenter\"][i].item(),\n",
    "                    \"dx\": batch[\"dx\"][i].item(),\n",
    "                    \"dy\": batch[\"dy\"][i].item(),\n",
    "                    \"jsn_score\": int(jsn_scores[i]),\n",
    "                    \"erosion_score\": int(erosion_scores[i]),\n",
    "                    \"PAD\": 0.0\n",
    "                })\n",
    "    return results\n",
    "\n",
    "# Main function\n",
    "@hydra.main(config_path=\"config/classification\", config_name=\"submit\")\n",
    "def main(cfg: DictConfig):\n",
    "\n",
    "    model = load_eval_model(cfg, cfg.inference.model_weights).to(cfg.inference.device).double()\n",
    "\n",
    "    # Define transforms\n",
    "    transform = transforms.Compose([\n",
    "        transforms.LoadImage(),\n",
    "        transforms.EnsureChannelFirst(),\n",
    "        DynamicSquarePad(),\n",
    "        transforms.Resize(\n",
    "          spatial_size=[224, 224],\n",
    "          mode=\"bilinear\"\n",
    "        ),\n",
    "        transforms.NormalizeIntensity()\n",
    "    ])\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = EvalImageDataset(cfg.inference.image_dir, cfg.inference.bbox_csv, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=cfg.inference.batch_size, shuffle=False)\n",
    "\n",
    "    # Predict scores\n",
    "    results = predict_scores(cfg, model, dataloader, cfg.inference.device)\n",
    "\n",
    "    # Save results to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv(cfg.inference.output_csv, index=False)\n",
    "    print(f\"Predictions saved to {cfg.inference.output_csv}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-03-11T15:45:35.639745Z",
     "iopub.status.busy": "2025-03-11T15:45:35.639418Z",
     "iopub.status.idle": "2025-03-11T15:45:35.796717Z",
     "shell.execute_reply": "2025-03-11T15:45:35.795665Z",
     "shell.execute_reply.started": "2025-03-11T15:45:35.639717Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls -l checkpoints/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:46:10.042564Z",
     "iopub.status.busy": "2025-03-11T15:46:10.042205Z",
     "iopub.status.idle": "2025-03-11T15:46:10.048949Z",
     "shell.execute_reply": "2025-03-11T15:46:10.048053Z",
     "shell.execute_reply.started": "2025-03-11T15:46:10.042517Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/config/classification/submit.yaml\n",
    "defaults:\n",
    "  - model: \"timm\"\n",
    "\n",
    "inference:\n",
    "  batch_size: 32\n",
    "  model_weights: /kaggle/working/checkpoints/classification/model_epoch:4_jsn_accuracy:0.6620927173848217_erosion_accuracy:0.44785529285529285.pth\n",
    "  bbox_csv: /kaggle/working/data/eval_detection_output/bbox_txts/all_bboxes.txt\n",
    "  image_dir: /kaggle/working/data/eval_cropped_images\n",
    "  output_csv: /kaggle/working/submit.csv\n",
    "  device: \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:46:10.703913Z",
     "iopub.status.busy": "2025-03-11T15:46:10.703603Z",
     "iopub.status.idle": "2025-03-11T15:46:10.709149Z",
     "shell.execute_reply": "2025-03-11T15:46:10.708457Z",
     "shell.execute_reply.started": "2025-03-11T15:46:10.703884Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%writefile submit.sh\n",
    "export PYTHONPATH=$PYTHONPATH:/kaggle/working\n",
    "echo \"Updated PYTHONPATH: $PYTHONPATH\"\n",
    "python3 submit.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-11T15:46:11.524045Z",
     "iopub.status.busy": "2025-03-11T15:46:11.523717Z",
     "iopub.status.idle": "2025-03-11T15:46:50.876989Z",
     "shell.execute_reply": "2025-03-11T15:46:50.875899Z",
     "shell.execute_reply.started": "2025-03-11T15:46:11.524014Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!chmod +x submit.sh\n",
    "!./submit.sh"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11336108,
     "sourceId": 92337,
     "sourceType": "competition"
    },
    {
     "datasetId": 6833038,
     "sourceId": 10981011,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
